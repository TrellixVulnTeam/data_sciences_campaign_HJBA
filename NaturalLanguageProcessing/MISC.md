MISC

# [Blogs]

# [Papers- Misc]
* [jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models](https://arxiv.org/abs/2003.02249) [[github](https://github.com/nyu-mll/jiant/)]
* [Cloze-driven Pretraining of Self-attention Networks](https://arxiv.org/abs/1903.07785)
* [Learning and Evaluating General Linguistic Intelligence](https://arxiv.org/abs/1901.11373)
* [To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://arxiv.org/abs/1903.05987) (ACL2019 WS)
* [Learning to Speak and Act in a Fantasy Text Adventure Game](https://www.aclweb.org/anthology/D19-1062/) (EMNLP2019)
* [Conditional BERT Contextual Augmentation](https://arxiv.org/abs/1812.06705)
* [Data Augmentation using Pre-trained Transformer Models](https://arxiv.org/abs/2003.02245)
* [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962) (ICLR2020)
* [Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models](https://openreview.net/forum?id=HkgaETNtDB) (ICLR2020)
* [A Mutual Information Maximization Perspective of Language Representation Learning](https://openreview.net/forum?id=Syx79eBKwr) (ICLR2020)
* [Is BERT Really Robust? Natural Language Attack on Text Classification and Entailment](https://arxiv.org/abs/1907.11932) (AAAI2020)
* [Thieves on Sesame Street! Model Extraction of BERT-based APIs](https://arxiv.org/abs/1910.12366) (ICLR2020)
* [Graph-Bert: Only Attention is Needed for Learning Graph Representations](https://arxiv.org/abs/2001.05140)
* [CodeBERT: A Pre-Trained Model for Programming and Natural Languages](https://arxiv.org/abs/2002.08155)
* [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https://arxiv.org/abs/2002.06305)
* [Extending Machine Language Models toward Human-Level Language Understanding](https://arxiv.org/abs/1912.05877)
* [Glyce: Glyph-vectors for Chinese Character Representations](https://arxiv.org/abs/1901.10125)
* [Back to the Future -- Sequential Alignment of Text Representations](https://arxiv.org/abs/1909.03464)
* [Improving Cuneiform Language Identification with BERT](https://www.aclweb.org/anthology/papers/W/W19/W19-1402/) (NAACL2019 WS)
* [BERT has a Moral Compass: Improvements of ethical and moral values of machines](https://arxiv.org/abs/1912.05238)
* [SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction](https://dl.acm.org/citation.cfm?id=3342186) (ACM-BCB2019)
* [On the comparability of Pre-trained Language Models](https://arxiv.org/abs/2001.00781)
* [Transformers: State-of-the-art Natural Language Processing](https://arxiv.org/abs/1910.03771) 
* [Jukebox: A Generative Model for Music](https://cdn.openai.com/papers/jukebox.pdf)  
* [WT5?! Training Text-to-Text Models to Explain their
Predictions](https://arxiv.org/pdf/2004.14546.pdf)  
* [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/pdf/2004.02349.pdf) [[github](https://github.com/google-research/tapas)]  
* [TABERT: Pretraining for Joint Understanding of
Textual and Tabular Data](https://arxiv.org/pdf/2005.08314.pdf)    
+ FNet: Mixing Tokens with Fourier Transforms, https://arxiv.org/abs/2105.03824
+ 



# [Codes]

