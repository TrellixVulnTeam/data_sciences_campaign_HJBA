BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805


# [Blogs]

# [References]

# [Codes]

## [Codes- Tensorflow]
+ BERT, https://github.com/google-research/bert
+ bert-as-service, https://github.com/hanxiao/bert-as-service
+ 

## [Codes- Keras]
+ Keras BERT, https://github.com/CyberZHG/keras-bert
+ bert4keras, https://github.com/bojone/bert4keras

## [Codes- PyTorch]
+ BERT-pytorch, https://github.com/codertimo/BERT-pytorch
+ BertSum This code is for paper Fine-tune BERT for Extractive Summarization, https://github.com/nlpyang/BertSum

## [Codes- Visualization]
+ BertViz BertViz is a tool for visualizing attention in the Transformer model, supporting most models from the transformers library (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, BART, etc.). It extends the Tensor2Tensor visualization tool by Llion Jones and the transformers library from HuggingFace., https://github.com/jessevig/bertviz
+ 



