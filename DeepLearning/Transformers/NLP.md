Transformers on NLP

# [Blogs]

## [Blogs- Text Generation]
+ Text Generation with Transformers in Python, https://www.thepythoncode.com/article/text-generation-with-transformers-in-python
+ How to Paraphrase Text using Transformers in Python, https://www.thepythoncode.com/article/paraphrase-text-using-transformers-in-python
+ Text Generation from Knowledge Graphs with Graph Transformers, https://arxiv.org/pdf/1904.02342.pdf
+ 


# [Papers]

## [Papers- Attention]
+ Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention, https://arxiv.org/abs/2112.03254

## [Papers- Transformers]
+ ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. https://arxiv.org/abs/1909.11942
+ 
* Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing, https://arxiv.org/abs/2101.03289
* Fastformer: Additive Attention is All You Need,https://arxiv.org/abs/2108.09084
* Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text  Models, https://arxiv.org/abs/2108.08877
* [Attention Is All You Need - arXiv 2017)](https://arxiv.org/abs/1706.03762)  
* [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - arXiv 2019)](https://arxiv.org/abs/1901.02860)  
* [Universal Transformers - ICLR 2019)](https://arxiv.org/abs/1807.03819) 
* [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - arXiv 2019)](https://arxiv.org/abs/1910.10683) 
* [Reformer: The Efficient Transformer - ICLR 2020)](https://arxiv.org/abs/2001.04451) 
* [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799) (ACL2019)
* [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) (ACL2019) [[github](https://github.com/kimiyoung/transformer-xl)]
* [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
* [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015) (EMNLP2019)
* [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)
* [The Evolved Transformer](https://arxiv.org/abs/1901.11117) (ICML2019)
* [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) (ICLR2020) [[github](https://github.com/google/trax/tree/master/trax/models/reformer)]
* [GRET: Global Representation Enhanced Transformer](https://arxiv.org/abs/2002.10101) (AAAI2020)
* [Transformer on a Diet](https://arxiv.org/abs/2002.06170) [[github](https://github.com/cgraywang/transformer-on-diet)]
* [Efficient Content-Based Sparse Attention with Routing Transformers](https://openreview.net/forum?id=B1gjs6EtDr)
* [BP-Transformer: Modelling Long-Range Context via Binary Partitioning](https://arxiv.org/abs/1911.04070)  
* [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf)   
* [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)  
* [UnifiedQA: Crossing Format Boundaries With a Single QA System](https://arxiv.org/pdf/2005.00700.pdf) [[github](https://github.com/allenai/unifiedqa)]  
* [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf) 
* Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting, https://arxiv.org/abs/2012.07436, Codes: https://github.com/zhouhaoyi/Informer2020
* The Evolved Transformer, https://arxiv.org/abs/1901.11117
* Adaptively Sparse Transformers, https://arxiv.org/abs/1909.00015
* AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, https://arxiv.org/abs/2108.05542
* FLAT: Chinese NER Using Flat-Lattice Transformer, https://arxiv.org/abs/2004.11795
* Sentence Transformers: Multilingual Sentence, Paragraph, and Image Embeddings using BERT & Co., https://github.com/UKPLab/sentence-transformers
* 

## [Papers- Machine Translation]
* [The Evolved Transformer - David R. So(2019)](https://arxiv.org/pdf/1901.11117.pdf)  


## [Papers- Text Generation]
+ Text Generation with Transformers in Python, https://www.thepythoncode.com/article/text-generation-with-transformers-in-python
+ Text Generation from Knowledge Graphs with Graph Transformers, https://arxiv.org/pdf/1904.02342.pdf
+ Modeling Graph Structure in Transformer for Better AMR-to-Text Generation, https://arxiv.org/pdf/1909.00136.pdf
+ 

## [Papers- Text Style Transfer]
+ Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation, https://arxiv.org/pdf/1905.05621.pdf
+ 

## [Papers- Long Document Classification]
+ Revisiting Transformer-based Models for Long Document Classification, https://arxiv.org/abs/2204.06683
+ 

## [Papers- MISC]
+ CHARFORMER: FAST CHARACTER TRANSFORMERS VIA GRADIENT-BASED SUBWORD TOKENIZATION, https://arxiv.org/pdf/2106.12672.pdf
+ 

# [Codes]

## [Codes- Text Generation]
+ Text Generation from Knowledge Graphs with Graph Transformers, https://github.com/rikdz/GraphWriter
+ Structural Transformer Model, https://github.com/Amazing-J/structural-transformer

# [ToolKits]
+ Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing, https://github.com/nlp-uoregon/trankit
+ 
