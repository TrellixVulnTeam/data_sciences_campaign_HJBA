Transformers on NLP

# [Blogs]

# [Papers]
* Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing, https://arxiv.org/abs/2101.03289
* Fastformer: Additive Attention is All You Need,https://arxiv.org/abs/2108.09084
* Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text  Models, https://arxiv.org/abs/2108.08877
* [Attention Is All You Need - arXiv 2017)](https://arxiv.org/abs/1706.03762)  
* [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - arXiv 2019)](https://arxiv.org/abs/1901.02860)  
* [Universal Transformers - ICLR 2019)](https://arxiv.org/abs/1807.03819) 
* [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - arXiv 2019)](https://arxiv.org/abs/1910.10683) 
* [Reformer: The Efficient Transformer - ICLR 2020)](https://arxiv.org/abs/2001.04451) 
* [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799) (ACL2019)
* [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) (ACL2019) [[github](https://github.com/kimiyoung/transformer-xl)]
* [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
* [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015) (EMNLP2019)
* [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)
* [The Evolved Transformer](https://arxiv.org/abs/1901.11117) (ICML2019)
* [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) (ICLR2020) [[github](https://github.com/google/trax/tree/master/trax/models/reformer)]
* [GRET: Global Representation Enhanced Transformer](https://arxiv.org/abs/2002.10101) (AAAI2020)
* [Transformer on a Diet](https://arxiv.org/abs/2002.06170) [[github](https://github.com/cgraywang/transformer-on-diet)]
* [Efficient Content-Based Sparse Attention with Routing Transformers](https://openreview.net/forum?id=B1gjs6EtDr)
* [BP-Transformer: Modelling Long-Range Context via Binary Partitioning](https://arxiv.org/abs/1911.04070)  
* [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf)   
* [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)  
* [UnifiedQA: Crossing Format Boundaries With a Single QA System](https://arxiv.org/pdf/2005.00700.pdf) [[github](https://github.com/allenai/unifiedqa)]  
* [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf) 
* Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting, https://arxiv.org/abs/2012.07436, Codes: https://github.com/zhouhaoyi/Informer2020
* The Evolved Transformer, https://arxiv.org/abs/1901.11117
* Adaptively Sparse Transformers, https://arxiv.org/abs/1909.00015
* AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, https://arxiv.org/abs/2108.05542
* FLAT: Chinese NER Using Flat-Lattice Transformer, https://arxiv.org/abs/2004.11795
* 

## [Papers- Machine Translation]
* [The Evolved Transformer - David R. So(2019)](https://arxiv.org/pdf/1901.11117.pdf)  



# [Codes]


# [ToolKits]
+ Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing, https://github.com/nlp-uoregon/trankit
+ 
