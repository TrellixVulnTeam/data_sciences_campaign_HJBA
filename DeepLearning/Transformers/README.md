Transformers

- - -

# [Courses]
+ Attention Is All You Need, https://www.youtube.com/watch?v=iDulhoQ2pro
+ 台大李宏毅21年机器学习课程 self-attention和transformer, https://www.bilibili.com/video/BV1Xp4y1b7ih/?spm_id_from=autoNext


# [Blogs]
+ multiscale-vision-transformers-an-architecture-for-modeling-visual-data, https://ai.facebook.com/blog/multiscale-vision-transformers-an-architecture-for-modeling-visual-data/
+ The Illustrated Transformer, https://jalammar.github.io/illustrated-transformer/
+ Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention),https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
+ Self-Attention和Transformer, https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer
+ A Complete Learning Path To Transformers (With Guide To 23 Architectures), https://analyticsindiamag.com/a-complete-learning-path-to-transformers/
+ The Essential Guide to Transformers, the Key to Modern SOTA AI, https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html
+ How Transformers work in deep learning and NLP: an intuitive introduction, https://theaisummer.com/transformer/
+ Comprehensive Guide To Transformers, https://analyticsindiamag.com/a-comprehensive-guide-to-transformers/
+ Zero-Shot Controlle Generation with Encoder-Decoder Transformers, https://www.amazon.science/latest-news/controlling-language-generation-models-without-training-data?utm_content=buffer9ee31&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer
+ This course will teach you about natural language processing (NLP) using libraries from the Hugging Face ecosystem — 🤗 Transformers, 🤗 Datasets, 🤗 Tokenizers, and 🤗 Accelerate — as well as the Hugging Face Hub. It’s completely free and without ads., https://huggingface.co/course/chapter1
+ Transformer在图像复原领域的降维打击！ETH提出SwinIR：各项任务全面领先, https://mp.weixin.qq.com/s/w28DdccPeW_2vl2S_t8sAA
+ Advancing the state of the art in computer vision with self-supervised Transformers and 10x more efficient training, https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training
+ 华为联合北大、悉尼大学对 Visual Transformer 的最新综述, https://zhuanlan.zhihu.com/p/339418612
+ 



# [Papers-Surveys]
+ A Survey of Transformers, https://arxiv.org/abs/2106.04554
+ Transformers in Vision: A Survey, https://arxiv.org/abs/2101.01169
+ A Survey on Vision Transformer, https://arxiv.org/abs/2012.12556
+ 

# [Papers- Computer Vision]
+ Vision Transformer with Progressive Sampling, https://arxiv.org/abs/2108.01684
+ ViTGAN: Training GANs with Vision Transformers, https://arxiv.org/abs/2107.04589
+ Do Vision Transformers See Like Convolutional Neural Networks? https://arxiv.org/abs/2108.08810
+ Focal Self-attention for Local-Global Interactions in Vision Transformers, https://arxiv.org/abs/2107.00641
+ Is it Time to Replace CNNs with Transformers for Medical Images? https://arxiv.org/abs/2108.09038
+ Video Relation Detection via Tracklet based Visual Transformer, https://arxiv.org/abs/2108.08669
+ [***Vision Transformer(ViT)***] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929
+ SwinIR: Image Restoration Using Swin Transformer, https://arxiv.org/abs/2108.10257
+ How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers, https://arxiv.org/abs/2106.10270
+ [moco v3] An Empirical Study of Training Self-Supervised Vision Transformers, https://arxiv.org/abs/2104.02057
+ [*Facebook*] Training data-efficient image transformers & distillation through attention, https://arxiv.org/abs/2012.12877
+ 

# [Papers- Object Detection]
+ [***DETR (DEtection TRansformer)***][***Facebook***] End-to-End Object Detection with Transformers, https://arxiv.org/abs/2005.12872, Codes: https://github.com/facebookresearch/detr
+ 
+ 

# [Papers- Image Super-Resolution]
+ Learning Texture Transformer Network for Image Super-Resolution, https://arxiv.org/abs/2006.04139
+ 

# [Papers- Image Segmentation]

## [Papers- Medical Image Segmentation]
+ TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation, https://arxiv.org/abs/2102.04306


# [Papers- NLP]
+ Fastformer: Additive Attention is All You Need,https://arxiv.org/abs/2108.09084
+ Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text  Models, https://arxiv.org/abs/2108.08877
+ Fastformer: Additive Attention Can Be All You Need, https://arxiv.org/abs/2108.09084
+ 


# [Papers- MISC]
+ Self-Attention with Relative Position Representations, https://arxiv.org/abs/1803.02155
+ An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929
+ TransformerFusion: Monocular RGB Scene Reconstruction using Transformers, https://arxiv.org/abs/2107.02191
+ BumbleBee: A Transformer for Music, https://arxiv.org/abs/2107.03443
+ Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation, https://github.com/NVIDIA-Merlin/publications/tree/main/2021_acm_recsys_transformers4rec
+ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805
+ Thinking Like Transformers, https://arxiv.org/abs/2106.06981
+ Mobile-Former: Bridging MobileNet and Transformer, https://arxiv.org/abs/2108.05895
+ 



# [Books]
+ ***Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques***

# [Codes]
+  Transformer的PyTorch实现， https://www.bilibili.com/video/BV1mk4y1q7eK?p=2， ，Transformer 的 PyTorch 实现， https://wmathor.com/index.php/archives/1455/
+  Transformers, https://github.com/huggingface/transformers
+  


