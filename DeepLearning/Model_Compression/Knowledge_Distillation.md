Knowledge distillation

# [Blogs]

# [Papers]
+ Knowledge distillation: A good teacher is patient and consistent, https://arxiv.org/abs/2106.05237
+ Symbolic Knowledge Distillation: from General Language Models to Commonsense Models, https://arxiv.org/abs/2110.07178
+ 

# [Codes]

